#include "ascon-select-backend.h"
#if defined(ASCON_BACKEND_I386)
/*
 * Copyright (C) 2022 Southern Storm Software, Pty Ltd.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

	.text

	.p2align 4,,15
#if defined(__CYGWIN32__) || defined(_WIN32)
	.globl	_ascon_permute
	.def	_ascon_permute;	.scl	2;	.type	32;	.endef
_ascon_permute:
#else
	.globl	ascon_permute
	.type	ascon_permute, @function
ascon_permute:
#endif
	pushl	%ebp
	pushl	%ebx
	pushl	%esi
	pushl	%edi
	subl	$48, %esp
	movl	68(%esp), %eax
	movl	72(%esp), %ebp
	movl	4(%eax), %ebx
	movl	12(%eax), %ecx
	movl	20(%eax), %edx
	movl	28(%eax), %esi
	movl	36(%eax), %edi
	notl	%edx
	movl	%ebx, 4(%esp)
	movl	%ecx, 12(%esp)
	movl	%edx, 20(%esp)
	movl	%esi, 28(%esp)
	movl	%edi, 36(%esp)
	movl	(%eax), %ebx
	movl	8(%eax), %ecx
	movl	16(%eax), %edx
	movl	24(%eax), %esi
	movl	32(%eax), %edi
	notl	%edx
	cmpl	$12, %ebp
	jge	.L13
	movl	.L14(,%ebp,4), %eax
	jmp	*%eax
.L13:
	jmp	.L12
	.section	.rodata
	.align	4
	.L14:
	.long	.L0
	.long	.L1
	.long	.L2
	.long	.L3
	.long	.L4
	.long	.L5
	.long	.L6
	.long	.L7
	.long	.L8
	.long	.L9
	.long	.L10
	.long	.L11
	.text
.L0:
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L1:
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L2:
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L3:
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L4:
	xorl	$-7, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L5:
	xorl	$-4, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L6:
	xorl	$-7, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L7:
	xorl	$-4, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L8:
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-7, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L9:
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-7, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L10:
	xorl	$-13, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-4, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L11:
	xorl	$-10, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%ebx, (%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 16(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 32(%esp)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	xorl	$-4, %edx
	xorl	%edi, %ebx
	xorl	%esi, %edi
	xorl	%ecx, %edx
	movl	%ebx, 44(%esp)
	movl	%ebx, %eax
	notl	%eax
	andl	%ecx, %eax
	movl	%eax, 40(%esp)
	movl	%ecx, %ebp
	movl	%edx, %eax
	notl	%ebp
	notl	%eax
	andl	%edx, %ebp
	andl	%esi, %eax
	xorl	%ebp, %ebx
	xorl	%eax, %ecx
	movl	%edi, %eax
	movl	44(%esp), %ebp
	notl	%eax
	andl	%ebp, %eax
	xorl	%eax, %esi
	movl	%esi, %ebp
	notl	%ebp
	andl	%edi, %ebp
	xorl	%ebp, %edx
	movl	40(%esp), %eax
	xorl	%eax, %edi
	xorl	%ebx, %ecx
	xorl	%edi, %ebx
	xorl	%edx, %esi
	movl	%edi, 40(%esp)
	movl	(%esp), %edi
	movl	%ebx, %eax
	movl	%edi, %ebp
	rorl	$4, %eax
	rorl	$5, %ebp
	xorl	%edi, %eax
	xorl	%ebx, %ebp
	rorl	$10, %eax
	rorl	$9, %ebp
	xorl	%ebx, %eax
	xorl	%ebp, %edi
	movl	%eax, 4(%esp)
	movl	%edi, %ebx
	movl	8(%esp), %edi
	movl	%ecx, %ebp
	movl	%edi, %eax
	rorl	$11, %ebp
	rorl	$11, %eax
	xorl	%ecx, %ebp
	xorl	%edi, %eax
	rorl	$19, %ebp
	rorl	$20, %eax
	xorl	%ebp, %edi
	xorl	%ecx, %eax
	movl	%edi, %ecx
	movl	%eax, 12(%esp)
	movl	16(%esp), %edi
	movl	%edx, %eax
	movl	%edi, %ebp
	rorl	$2, %eax
	rorl	$3, %ebp
	xorl	%edi, %eax
	xorl	%edx, %ebp
	rorl	$1, %eax
	xorl	%ebp, %edi
	xorl	%edx, %eax
	movl	%edi, %edx
	movl	%eax, 20(%esp)
	movl	24(%esp), %edi
	movl	%esi, %eax
	movl	%edi, %ebp
	rorl	$3, %eax
	rorl	$4, %ebp
	xorl	%edi, %eax
	xorl	%esi, %ebp
	rorl	$5, %eax
	rorl	$5, %ebp
	xorl	%eax, %edi
	xorl	%esi, %ebp
	movl	%edi, %esi
	movl	%ebp, 28(%esp)
	movl	40(%esp), %edi
	movl	%ebx, 40(%esp)
	movl	32(%esp), %ebx
	movl	%edi, %ebp
	movl	%ebx, %eax
	rorl	$17, %ebp
	rorl	$17, %eax
	xorl	%edi, %ebp
	xorl	%ebx, %eax
	rorl	$3, %ebp
	rorl	$4, %eax
	xorl	%ebp, %ebx
	xorl	%edi, %eax
	movl	%ebx, %edi
	movl	%eax, 36(%esp)
	movl	40(%esp), %ebx
.L12:
	movl	68(%esp), %eax
	notl	%edx
	movl	%ebx, (%eax)
	movl	%ecx, 8(%eax)
	movl	%edx, 16(%eax)
	movl	%esi, 24(%eax)
	movl	%edi, 32(%eax)
	movl	4(%esp), %ebx
	movl	12(%esp), %ecx
	movl	20(%esp), %edx
	movl	28(%esp), %esi
	movl	36(%esp), %edi
	notl	%edx
	movl	%ebx, 4(%eax)
	movl	%ecx, 12(%eax)
	movl	%edx, 20(%eax)
	movl	%esi, 28(%eax)
	movl	%edi, 36(%eax)
	addl	$48, %esp
	popl	%edi
	popl	%esi
	popl	%ebx
	popl	%ebp
	ret
#if !(defined(__CYGWIN32__) || defined(_WIN32))
	.size	ascon_permute, .-ascon_permute
#endif

	.p2align 4,,15
#if defined(__CYGWIN32__) || defined(_WIN32)
	.globl	_ascon_from_regular
	.def	_ascon_from_regular;	.scl	2;	.type	32;	.endef
_ascon_from_regular:
#else
	.globl	ascon_from_regular
	.type	ascon_from_regular, @function
ascon_from_regular:
#endif
	pushl	%edi
	pushl	%esi
	movl	12(%esp), %eax
	movl	(%eax), %ecx
	movl	4(%eax), %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$1, %edi
	shrl	$1, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$572662306, %edi
	andl	$572662306, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$1, %edi
	shll	$1, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$2, %edi
	shrl	$2, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$202116108, %edi
	andl	$202116108, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$2, %edi
	shll	$2, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$983055, %edi
	andl	$983055, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shll	$16, %edi
	andl	$65535, %esi
	orl	%edi, %esi
	andl	$-65536, %ecx
	shrl	$16, %edx
	orl	%ecx, %edx
	movl	%esi, (%eax)
	movl	%edx, 4(%eax)
	movl	8(%eax), %ecx
	movl	12(%eax), %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$1, %edi
	shrl	$1, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$572662306, %edi
	andl	$572662306, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$1, %edi
	shll	$1, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$2, %edi
	shrl	$2, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$202116108, %edi
	andl	$202116108, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$2, %edi
	shll	$2, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$983055, %edi
	andl	$983055, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shll	$16, %edi
	andl	$65535, %esi
	orl	%edi, %esi
	andl	$-65536, %ecx
	shrl	$16, %edx
	orl	%ecx, %edx
	movl	%esi, 8(%eax)
	movl	%edx, 12(%eax)
	movl	16(%eax), %ecx
	movl	20(%eax), %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$1, %edi
	shrl	$1, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$572662306, %edi
	andl	$572662306, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$1, %edi
	shll	$1, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$2, %edi
	shrl	$2, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$202116108, %edi
	andl	$202116108, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$2, %edi
	shll	$2, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$983055, %edi
	andl	$983055, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shll	$16, %edi
	andl	$65535, %esi
	orl	%edi, %esi
	andl	$-65536, %ecx
	shrl	$16, %edx
	orl	%ecx, %edx
	movl	%esi, 16(%eax)
	movl	%edx, 20(%eax)
	movl	24(%eax), %ecx
	movl	28(%eax), %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$1, %edi
	shrl	$1, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$572662306, %edi
	andl	$572662306, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$1, %edi
	shll	$1, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$2, %edi
	shrl	$2, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$202116108, %edi
	andl	$202116108, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$2, %edi
	shll	$2, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$983055, %edi
	andl	$983055, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shll	$16, %edi
	andl	$65535, %esi
	orl	%edi, %esi
	andl	$-65536, %ecx
	shrl	$16, %edx
	orl	%ecx, %edx
	movl	%esi, 24(%eax)
	movl	%edx, 28(%eax)
	movl	32(%eax), %ecx
	movl	36(%eax), %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$1, %edi
	shrl	$1, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$572662306, %edi
	andl	$572662306, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$1, %edi
	shll	$1, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$2, %edi
	shrl	$2, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$202116108, %edi
	andl	$202116108, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$2, %edi
	shll	$2, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$983055, %edi
	andl	$983055, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shll	$16, %edi
	andl	$65535, %esi
	orl	%edi, %esi
	andl	$-65536, %ecx
	shrl	$16, %edx
	orl	%ecx, %edx
	movl	%esi, 32(%eax)
	movl	%edx, 36(%eax)
	popl	%esi
	popl	%edi
	ret
#if !(defined(__CYGWIN32__) || defined(_WIN32))
	.size	ascon_from_regular, .-ascon_from_regular
#endif

	.p2align 4,,15
#if defined(__CYGWIN32__) || defined(_WIN32)
	.globl	_ascon_to_regular
	.def	_ascon_to_regular;	.scl	2;	.type	32;	.endef
_ascon_to_regular:
#else
	.globl	ascon_to_regular
	.type	ascon_to_regular, @function
ascon_to_regular:
#endif
	pushl	%edi
	pushl	%esi
	movl	12(%esp), %eax
	movl	(%eax), %edx
	movl	4(%eax), %ecx
	movl	%edx, %edi
	movl	%ecx, %esi
	shrl	$16, %edi
	shll	$16, %esi
	andl	$65535, %edx
	andl	$-65536, %ecx
	orl	%esi, %edx
	orl	%edi, %ecx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$15, %edi
	shrl	$15, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$43690, %edi
	andl	$43690, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$15, %edi
	shll	$15, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$14, %edi
	shrl	$14, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$52428, %edi
	andl	$52428, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$14, %edi
	shll	$14, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$61680, %edi
	andl	$61680, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, (%eax)
	movl	%edx, 4(%eax)
	movl	8(%eax), %edx
	movl	12(%eax), %ecx
	movl	%edx, %edi
	movl	%ecx, %esi
	shrl	$16, %edi
	shll	$16, %esi
	andl	$65535, %edx
	andl	$-65536, %ecx
	orl	%esi, %edx
	orl	%edi, %ecx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$15, %edi
	shrl	$15, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$43690, %edi
	andl	$43690, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$15, %edi
	shll	$15, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$14, %edi
	shrl	$14, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$52428, %edi
	andl	$52428, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$14, %edi
	shll	$14, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$61680, %edi
	andl	$61680, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, 8(%eax)
	movl	%edx, 12(%eax)
	movl	16(%eax), %edx
	movl	20(%eax), %ecx
	movl	%edx, %edi
	movl	%ecx, %esi
	shrl	$16, %edi
	shll	$16, %esi
	andl	$65535, %edx
	andl	$-65536, %ecx
	orl	%esi, %edx
	orl	%edi, %ecx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$15, %edi
	shrl	$15, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$43690, %edi
	andl	$43690, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$15, %edi
	shll	$15, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$14, %edi
	shrl	$14, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$52428, %edi
	andl	$52428, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$14, %edi
	shll	$14, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$61680, %edi
	andl	$61680, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, 16(%eax)
	movl	%edx, 20(%eax)
	movl	24(%eax), %edx
	movl	28(%eax), %ecx
	movl	%edx, %edi
	movl	%ecx, %esi
	shrl	$16, %edi
	shll	$16, %esi
	andl	$65535, %edx
	andl	$-65536, %ecx
	orl	%esi, %edx
	orl	%edi, %ecx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$15, %edi
	shrl	$15, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$43690, %edi
	andl	$43690, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$15, %edi
	shll	$15, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$14, %edi
	shrl	$14, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$52428, %edi
	andl	$52428, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$14, %edi
	shll	$14, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$61680, %edi
	andl	$61680, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, 24(%eax)
	movl	%edx, 28(%eax)
	movl	32(%eax), %edx
	movl	36(%eax), %ecx
	movl	%edx, %edi
	movl	%ecx, %esi
	shrl	$16, %edi
	shll	$16, %esi
	andl	$65535, %edx
	andl	$-65536, %ecx
	orl	%esi, %edx
	orl	%edi, %ecx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$15, %edi
	shrl	$15, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$43690, %edi
	andl	$43690, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$15, %edi
	shll	$15, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$14, %edi
	shrl	$14, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$52428, %edi
	andl	$52428, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$14, %edi
	shll	$14, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$12, %edi
	shrl	$12, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$61680, %edi
	andl	$61680, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$12, %edi
	shll	$12, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, %edi
	movl	%edx, %esi
	shrl	$24, %edi
	shrl	$24, %esi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	andl	$255, %edi
	andl	$255, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	shll	$24, %edi
	shll	$24, %esi
	xorl	%edi, %ecx
	xorl	%esi, %edx
	movl	%ecx, 32(%eax)
	movl	%edx, 36(%eax)
	popl	%esi
	popl	%edi
	ret
#if !(defined(__CYGWIN32__) || defined(_WIN32))
	.size	ascon_to_regular, .-ascon_to_regular
#endif

#endif
